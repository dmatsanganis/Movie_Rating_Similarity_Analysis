{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d5433c",
   "metadata": {},
   "source": [
    "## Mining Big Datasets - Data Mining Assignment 1\n",
    "\n",
    "### Assignment 1: User Similarity using Jaccard Distance, Min Hashing, and LSH\n",
    "\n",
    "In this assignment, we will explore the use of Jaccard distance, min hashing, and Locality Sensitive Hashing (LSH) in the context of user similarity in a movie rating dataset. The dataset used for this assignment contains 100,000 ratings from 943 users on 1,682 movies. We will perform various tasks to compute user similarity and evaluate the effectiveness of different techniques. The tasks include importing and pre-processing the dataset, computing the exact Jaccard similarity of users, computing similarity using Min-hash signatures, and locating similar users using LSH index.\n",
    "\n",
    "While the current analysis will be done on [Jupyter Notebook](http://jupyter.org/) and in [Python 3.10.0](https://www.python.org/downloads/release/python-3100/).\n",
    "\n",
    "---\n",
    "\n",
    "####  Team members:\n",
    "\n",
    "---\n",
    "> Dimitrios Matsanganis <br />\n",
    "> Academic ID: f2822212 <br />\n",
    "> MSc Business Analytics 2022-2023 FT <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> dmatsanganis@gmail.com, dim.matsanganis@aueb.gr\n",
    "\n",
    "---\n",
    "\n",
    "> Foteini Nefeli Nouskali <br />\n",
    "> Academic ID: f2822213 <br />\n",
    "> MSc Business Analytics 2022-2023 FT <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> fn.nouskali@gmail.com, fot.nouskali@aueb.gr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c056c",
   "metadata": {},
   "source": [
    "### Task 1: Import and Pre-process the Dataset with Users\n",
    "---\n",
    "\n",
    "We will start by downloading the movieLens dataset, which includes information about users, movies, and ratings. The dataset consists of three files: \n",
    "- users.txt \n",
    "- movies.txt\n",
    "- ratings.txt\n",
    "\n",
    "The users.txt file contains user-related information such as user ID, age, gender, occupation, and postcode. The movies.txt file contains movie-related information including movie ID, title, and other details. The ratings.txt file contains user IDs, movie IDs, ratings (on a scale of 1-5), and timestamps.\n",
    "\n",
    "For this assignment, we will focus on the set of movies that each user has rated and not the actual ratings. We will preprocess the dataset by extracting the movie IDs for each user and organizing the data in a suitable format for further analysis. Any necessary conversions or processing steps will be described in detail, along with the reasons behind them.\n",
    "\n",
    "As the initial step, we import the [Pandas](https://pandas.pydata.org/) library, which is a powerful data manipulation and analysis library in Python. We will use it to read and process the dataset. Therefore, the first step is to import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d2a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f24a1",
   "metadata": {},
   "source": [
    "The second step is to read the dataset files using pd.read_csv function from pandas. We provide the file names and the appropriate separator (`|` for users.txt and movies.txt, and `\\t` for ratings.txt) and specify column names for each dataframe. \n",
    "\n",
    "The names parameter is used to assign column names to the dataframes. We store the information from each file in separate dataframes (users_df, movies_df, and ratings_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7699eb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>925</td>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>salesman</td>\n",
       "      <td>49036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>none</td>\n",
       "      <td>39042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>346</td>\n",
       "      <td>34</td>\n",
       "      <td>M</td>\n",
       "      <td>other</td>\n",
       "      <td>76059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>360</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>other</td>\n",
       "      <td>98027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>509</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>administrator</td>\n",
       "      <td>10011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>administrator</td>\n",
       "      <td>78213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>848</td>\n",
       "      <td>46</td>\n",
       "      <td>M</td>\n",
       "      <td>engineer</td>\n",
       "      <td>02146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>programmer</td>\n",
       "      <td>55106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>436</td>\n",
       "      <td>30</td>\n",
       "      <td>F</td>\n",
       "      <td>administrator</td>\n",
       "      <td>17345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>159</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>student</td>\n",
       "      <td>55346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userid  age gender     occupation postcode\n",
       "924     925   18      F       salesman    49036\n",
       "255     256   35      F           none    39042\n",
       "345     346   34      M          other    76059\n",
       "359     360   51      M          other    98027\n",
       "508     509   23      M  administrator    10011\n",
       "399     400   33      F  administrator    78213\n",
       "847     848   46      M       engineer    02146\n",
       "299     300   26      F     programmer    55106\n",
       "435     436   30      F  administrator    17345\n",
       "158     159   23      F        student    55346"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read users.txt and store it as users_df.\n",
    "users_df = pd.read_csv('users.txt', sep='|', header=None, names=['userid', 'age', 'gender', 'occupation', 'postcode'])\n",
    "\n",
    "# Preview a sample of the dataframe.\n",
    "users_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60d6079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieid</th>\n",
       "      <th>movie_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>362</td>\n",
       "      <td>Blues Brothers 2000 (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>756</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1300</td>\n",
       "      <td>'Til There Was You (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>336</td>\n",
       "      <td>Playing God (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>174</td>\n",
       "      <td>Raiders of the Lost Ark (1981)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>502</td>\n",
       "      <td>Bananas (1971)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>Gone with the Wind (1939)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1183</td>\n",
       "      <td>Cowboy Way, The (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1208</td>\n",
       "      <td>Kiss of Death (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1279</td>\n",
       "      <td>Wild America (1997)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieid                         movie_title\n",
       "361       362          Blues Brothers 2000 (1998)\n",
       "755       756  Father of the Bride Part II (1995)\n",
       "1299     1300           'Til There Was You (1997)\n",
       "335       336                  Playing God (1997)\n",
       "173       174      Raiders of the Lost Ark (1981)\n",
       "501       502                      Bananas (1971)\n",
       "132       133           Gone with the Wind (1939)\n",
       "1182     1183              Cowboy Way, The (1994)\n",
       "1207     1208                Kiss of Death (1995)\n",
       "1278     1279                 Wild America (1997)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read movies.txt and store it as movies_df.\n",
    "movies_df = pd.read_csv('movies.txt', sep='|', header=None, encoding='latin-1', usecols=[0, 1])\n",
    "\n",
    "# Rename the columns to 'movieid' and 'movie_title'.\n",
    "movies_df.columns = ['movieid', 'movie_title']\n",
    "\n",
    "# Preview a sample of the dataframe.\n",
    "movies_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08838b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48237</th>\n",
       "      <td>655</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "      <td>888474713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81873</th>\n",
       "      <td>425</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>878737981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31818</th>\n",
       "      <td>417</td>\n",
       "      <td>855</td>\n",
       "      <td>2</td>\n",
       "      <td>879647450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91511</th>\n",
       "      <td>497</td>\n",
       "      <td>1041</td>\n",
       "      <td>3</td>\n",
       "      <td>879310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16020</th>\n",
       "      <td>303</td>\n",
       "      <td>384</td>\n",
       "      <td>3</td>\n",
       "      <td>879485165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91541</th>\n",
       "      <td>149</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>883512591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54994</th>\n",
       "      <td>675</td>\n",
       "      <td>937</td>\n",
       "      <td>1</td>\n",
       "      <td>889490151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70121</th>\n",
       "      <td>712</td>\n",
       "      <td>486</td>\n",
       "      <td>4</td>\n",
       "      <td>874730521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34552</th>\n",
       "      <td>254</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>886474518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37050</th>\n",
       "      <td>378</td>\n",
       "      <td>365</td>\n",
       "      <td>2</td>\n",
       "      <td>880318158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  movieid  rating  timestamp\n",
       "48237     655      740       3  888474713\n",
       "81873     425       11       3  878737981\n",
       "31818     417      855       2  879647450\n",
       "91511     497     1041       3  879310473\n",
       "16020     303      384       3  879485165\n",
       "91541     149      333       1  883512591\n",
       "54994     675      937       1  889490151\n",
       "70121     712      486       4  874730521\n",
       "34552     254       21       3  886474518\n",
       "37050     378      365       2  880318158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read ratings.txt and store it as ratings_df.\n",
    "ratings_df = pd.read_csv('ratings.txt', sep='\\t', header=None, names=['userid', 'movieid', 'rating', 'timestamp'])\n",
    "\n",
    "# Preview a sample of the dataframe.\n",
    "ratings_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6af59",
   "metadata": {},
   "source": [
    "Reading the `movies_df` we encountered an issue regarding non-standard characters in the file. To handle this, we used the encoding parameter (`latin-1`), which is used because it supports a wider range of characters compared to the default UTF-8 encoding. This ensures that any non-standard characters in the file are handled correctly during the reading process. \n",
    "\n",
    "Then, the `usecols=[0, 1]` parameter is used to select only the first two columns from the file. This is done to focus on the columns that are important for the analysis, discarding any additional columns that are not required - following the assignmen's instructions. After reading the file, the column names are updated to `movieid` and `movie_title`.\n",
    "\n",
    "For the other two datasets `users.txt` and `ratings.txt`, no encoding issues were encountered because they didn't contain characters incompatible with the default UTF-8 encoding. Hence, the `latin-1` encoding was not required for those datasets. \n",
    "\n",
    "Finally, all three datasets `users.txt`, `movies.txt`, and `ratings.txt` have been successfully loaded and previewed according to the provided instructions.\n",
    "\n",
    "Then, we create the user_movie_df dataframe by selecting only the relevant columns (userid and movieid) from ratings_df and dropping any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d4e795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20201</th>\n",
       "      <td>250</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90010</th>\n",
       "      <td>734</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17542</th>\n",
       "      <td>374</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17360</th>\n",
       "      <td>195</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>655</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27550</th>\n",
       "      <td>390</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87058</th>\n",
       "      <td>815</td>\n",
       "      <td>1204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75492</th>\n",
       "      <td>134</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29275</th>\n",
       "      <td>358</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77788</th>\n",
       "      <td>271</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  movieid\n",
       "20201     250       50\n",
       "90010     734      164\n",
       "17542     374      552\n",
       "17360     195      271\n",
       "49851     655      320\n",
       "27550     390      319\n",
       "87058     815     1204\n",
       "75492     134      678\n",
       "29275     358      127\n",
       "77788     271      178"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the relevant columns from ratings_df (userid and movieid) and drop duplicates.\n",
    "ratings_df = ratings_df[['userid', 'movieid']]\n",
    "\n",
    "# Preview a sample of the dataframe.\n",
    "ratings_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc0020",
   "metadata": {},
   "source": [
    "Now we will merge the three dataframes `users_df`, `movies_df`, and `ratings_df` into a single dataframe by combining the relevant columns from each dataframe. The common column between these dataframes is `userid` from users_df and `movieid` from movies_df and ratings_df. The resulting dataframe will be named `merged_df`.\n",
    "\n",
    "To be more precise, first, we merge the ratings_df dataframe with the users_df dataframe. We select specific columns (userid, gender, age, occupation, and postcode) from the users_df dataframe to include in the merged dataframe. The merging is performed based on the common column 'userid'.\n",
    "\n",
    "Next, we merge the resulting dataframe with the movies_df dataframe. We select the columns 'movieid' and 'movie_title' from the movies_df dataframe to include in the merged dataframe. The merging is again performed based on the common column 'movieid'.\n",
    "\n",
    "The merged dataframe merged_df now contains a combination of information from all three original dataframes. It includes columns such as 'userid', 'gender', 'age', 'occupation', 'postcode', 'movieid', and 'movie_title'.\n",
    "\n",
    "To ensure the correctness of the merging process, we preview a sample of 10 randomly selected rows from the merged_df dataframe. This provides a glimpse of the merged data and allows us to verify that the merging operation was successful.\n",
    "\n",
    "By merging the dataframes, we create a consolidated dataset that brings together user information, movie details, and corresponding ratings. This combined dataset can be used for further analysis and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe8f10f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>postcode</th>\n",
       "      <th>movie_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>190</td>\n",
       "      <td>291</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>administrator</td>\n",
       "      <td>95938</td>\n",
       "      <td>Absolute Power (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49381</th>\n",
       "      <td>279</td>\n",
       "      <td>763</td>\n",
       "      <td>M</td>\n",
       "      <td>33</td>\n",
       "      <td>programmer</td>\n",
       "      <td>85251</td>\n",
       "      <td>Happy Gilmore (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27051</th>\n",
       "      <td>246</td>\n",
       "      <td>451</td>\n",
       "      <td>M</td>\n",
       "      <td>19</td>\n",
       "      <td>student</td>\n",
       "      <td>28734</td>\n",
       "      <td>Grease (1978)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48855</th>\n",
       "      <td>233</td>\n",
       "      <td>197</td>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "      <td>engineer</td>\n",
       "      <td>98682</td>\n",
       "      <td>Graduate, The (1967)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36104</th>\n",
       "      <td>587</td>\n",
       "      <td>310</td>\n",
       "      <td>M</td>\n",
       "      <td>26</td>\n",
       "      <td>other</td>\n",
       "      <td>14216</td>\n",
       "      <td>Rainmaker, The (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92223</th>\n",
       "      <td>113</td>\n",
       "      <td>292</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>executive</td>\n",
       "      <td>95032</td>\n",
       "      <td>Rosewood (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42070</th>\n",
       "      <td>738</td>\n",
       "      <td>69</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>technician</td>\n",
       "      <td>95403</td>\n",
       "      <td>Forrest Gump (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36719</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>educator</td>\n",
       "      <td>29206</td>\n",
       "      <td>Twelve Monkeys (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86363</th>\n",
       "      <td>894</td>\n",
       "      <td>1381</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>educator</td>\n",
       "      <td>74075</td>\n",
       "      <td>Losing Chase (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74192</th>\n",
       "      <td>305</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>programmer</td>\n",
       "      <td>94086</td>\n",
       "      <td>Three Colors: Blue (1993)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  movieid gender  age     occupation postcode  \\\n",
       "16771     190      291      M   30  administrator    95938   \n",
       "49381     279      763      M   33     programmer    85251   \n",
       "27051     246      451      M   19        student    28734   \n",
       "48855     233      197      M   38       engineer    98682   \n",
       "36104     587      310      M   26          other    14216   \n",
       "92223     113      292      M   47      executive    95032   \n",
       "42070     738       69      M   35     technician    95403   \n",
       "36719      13        7      M   47       educator    29206   \n",
       "86363     894     1381      M   47       educator    74075   \n",
       "74192     305       60      M   23     programmer    94086   \n",
       "\n",
       "                     movie_title  \n",
       "16771      Absolute Power (1997)  \n",
       "49381       Happy Gilmore (1996)  \n",
       "27051              Grease (1978)  \n",
       "48855       Graduate, The (1967)  \n",
       "36104      Rainmaker, The (1997)  \n",
       "92223            Rosewood (1997)  \n",
       "42070        Forrest Gump (1994)  \n",
       "36719      Twelve Monkeys (1995)  \n",
       "86363        Losing Chase (1996)  \n",
       "74192  Three Colors: Blue (1993)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the dataframes.\n",
    "merged_df = pd.merge(ratings_df, users_df[['userid', 'gender', 'age', 'occupation', 'postcode']], on='userid')\n",
    "merged_df = pd.merge(merged_df, movies_df[['movieid', 'movie_title']], on='movieid')\n",
    "\n",
    "# Preview a sample of the merged dataframe.\n",
    "merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bef2a0",
   "metadata": {},
   "source": [
    "### Task 2: Compute Exact Jaccard Similarity of Users\n",
    "---\n",
    "\n",
    "To assess the similarity between users, we will compute the exact Jaccard Similarity for all pairs of users. The Jaccard Similarity measures the similarity between two sets by calculating the size of their intersection divided by the size of their union. We will compute the Jaccard Similarity for all unique pairs of users and output only those pairs with a similarity score of at least 0.5 (50% or higher). Additionally, we will identify the movie titles that the most similar pair of users have both seen.\n",
    "\n",
    "To be more preceise, the first step is to create a set of movies for each user. We can do so if we iterate through the merged dataframe from Task 1 (merged_df) and for each unique user, create a set of movies they have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4c1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of movies for each user.\n",
    "user_movies = {}\n",
    "\n",
    "# A for-loop statement to implement the iteratively procedure.\n",
    "for row in merged_df.itertuples():\n",
    "    userid = getattr(row, 'userid')\n",
    "    movieid = getattr(row, 'movieid')\n",
    "    \n",
    "    if userid not in user_movies:\n",
    "        user_movies[userid] = set()\n",
    "    user_movies[userid].add(movieid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b55b5",
   "metadata": {},
   "source": [
    "The next step is to compute Jaccard similarity for each pair of users. Again we will iterate through all possible pairs of users and calculate their Jaccard similarity using the formula: \n",
    "\n",
    "`Jaccard Similarity = Intersection / Union`, \n",
    "\n",
    "where `Intersection` is the number of movies both users have seen and `Union` is the total number of unique movies seen by both users.\n",
    "\n",
    "Finally, we will store the pairs of users with a similarity score of at least 0.5. Thus, through an if-condition statement we will identify the pairs of users that have a Jaccard similarity of at least 0.5 (50%) and store their userids and the corresponding similarity score. Meanwhile, we take care of the duplicate reversed pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608de67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users' pairs with Jaccard similarity index above 0.5 is 10 and are presented below:\n",
      "\n",
      "User 408 and User 898: 0.8387096774193549\n",
      "User 328 and User 788: 0.6729559748427673\n",
      "User 489 and User 587: 0.6299212598425197\n",
      "User 600 and User 826: 0.5454545454545454\n",
      "User 451 and User 489: 0.5333333333333333\n",
      "User 674 and User 879: 0.5217391304347826\n",
      "User 554 and User 764: 0.5170068027210885\n",
      "User 197 and User 826: 0.512987012987013\n",
      "User 197 and User 600: 0.5\n",
      "User 879 and User 800: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Through this inner for-loop statement we aim to compute Jaccard similarity for each pair of users.\n",
    "similar_pairs = []\n",
    "duplicates = set()\n",
    "\n",
    "for user1 in user_movies:\n",
    "    \n",
    "    for user2 in user_movies:\n",
    "        \n",
    "        if user1 != user2 and (user2, user1) not in duplicates:\n",
    "            movies1 = user_movies[user1]\n",
    "            movies2 = user_movies[user2]\n",
    "            \n",
    "            intersection = len(movies1.intersection(movies2))\n",
    "            union = len(movies1.union(movies2))\n",
    "            \n",
    "            jaccard_similarity = intersection / union\n",
    "            \n",
    "            # An If-condition to store the pairs with similarity >= 0.5.\n",
    "            if jaccard_similarity >= 0.5:\n",
    "                similar_pairs.append((user1, user2, jaccard_similarity))\n",
    "                duplicates.add((user1, user2))\n",
    "                \n",
    "# Print the total number of similar pairs with Jaccardi above 50% existing in the list. \n",
    "print(\"The number of users' pairs with Jaccard similarity index above 0.5 is\", len(similar_pairs), \"and are presented below:\")\n",
    "print()\n",
    "\n",
    "# Sort the pairs based on the Jaccard score.\n",
    "similar_pairs = sorted(similar_pairs, key=lambda x: x[2], reverse = True)\n",
    "\n",
    "# Preview the results.\n",
    "for i in similar_pairs:\n",
    "    print(\"User {} and User {}: {}\".format(i[0], i[1], i[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39859d4",
   "metadata": {},
   "source": [
    "As we can see from the above output, there are 10 pairs of user with Jaccard similarity over 0.5 value.\n",
    "\n",
    "The final step to conclude this task is to identify the pair of users with the highest similarity score and output the results properly. To do so we identified the pair of users with the highest Jaccard similarity score from the `similar_pairs` list. The `max()` function is used with a lambda function as the key to compare the similarity scores and select the maximum value. The most similar pair is stored in `most_similar_pair`, and the user IDs and similarity score are assigned to `user1_id`, `user2_id`, and `similarity_score`, respectively.\n",
    "\n",
    "Then, to find the movie titles that the most similar pair of users has seen, we retrieved the set of movies seen by each user using their respective IDs - `user1_id` and `user2_id` - from the `user_movies` dictionary. The `intersection()` method is used to find the **common movies between the two sets**, and the result is stored in `common_movies`. Finally, we printed the information of the most similar pair of users and the movie titles they have seen in common, in a readable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13eb8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Pair of Users:\n",
      "\n",
      "User 1 ID: 408\n",
      "User 2 ID: 898\n",
      "Jaccard Similarity Score: 0.8387096774193549\n",
      "\n",
      "Common Movie Titles:\n",
      "  - Contact (1997)\n",
      "  - Gattaca (1997)\n",
      "  - Starship Troopers (1997)\n",
      "  - Good Will Hunting (1997)\n",
      "  - Indian Summer (1996)\n",
      "  - Mouse Hunt (1997)\n",
      "  - English Patient, The (1996)\n",
      "  - Scream (1996)\n",
      "  - Rocket Man (1997)\n",
      "  - Air Force One (1997)\n",
      "  - L.A. Confidential (1997)\n",
      "  - Jackal, The (1997)\n",
      "  - Rainmaker, The (1997)\n",
      "  - Midnight in the Garden of Good and Evil (1997)\n",
      "  - Titanic (1997)\n",
      "  - Apt Pupil (1998)\n",
      "  - Everyone Says I Love You (1996)\n",
      "  - Lost Highway (1997)\n",
      "  - Cop Land (1997)\n",
      "  - Conspiracy Theory (1997)\n",
      "  - U Turn (1997)\n",
      "  - Wag the Dog (1997)\n",
      "  - Spawn (1997)\n",
      "  - Saint, The (1997)\n",
      "  - Tomorrow Never Dies (1997)\n",
      "  - Kolya (1996)\n"
     ]
    }
   ],
   "source": [
    "# Find the most similar pair of users.\n",
    "most_similar_pair = max(similar_pairs, key=lambda x: x[2])\n",
    "user1_id, user2_id, similarity_score = most_similar_pair\n",
    "\n",
    "# Find the movie titles that the most similar pair of users has seen.\n",
    "user1_movies = user_movies[user1_id]\n",
    "user2_movies = user_movies[user2_id]\n",
    "common_movies = user1_movies.intersection(user2_movies)\n",
    "\n",
    "# Output the results.\n",
    "print(\"Most Similar Pair of Users:\\n\")\n",
    "print(\"User 1 ID:\", user1_id)\n",
    "print(\"User 2 ID:\", user2_id)\n",
    "print(\"Jaccard Similarity Score:\", similarity_score)\n",
    "print(\"\\nCommon Movie Titles:\")\n",
    "for movie_id in common_movies:\n",
    "    movie_title = movies_df[movies_df['movieid'] == movie_id]['movie_title'].values[0]\n",
    "    print(\"  -\", movie_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8799a9c",
   "metadata": {},
   "source": [
    "The results of the analysis reveal the most similar pair of users based on the Jaccard similarity score and the movie titles they have seen in common. The most similar pair of users consists of **User 1 with ID 408 and User 2 with ID 898**. Their Jaccard similarity score is calculated to be approximately **0.8387, indicating a high degree of similarity in their movie preferences, between these 2 users**.\n",
    "\n",
    "These two users share a substantial number of movies in common, suggesting similar tastes in movies. The common movie titles include \"Contact\" (1997), \"Gattaca\" (1997), \"Starship Troopers\" (1997), \"Good Will Hunting\" (1997), \"Titanic\" (1997), and many others. This indicates that User 1 (ID 408) and User 2 (ID 898) have likely watched and enjoyed these movies, indicating potential similar preferences and interests in the movie domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9adad",
   "metadata": {},
   "source": [
    "### Task 3: Compute Similarity using Min-Hash Signatures\n",
    "---\n",
    "\n",
    "In this step, we will compute Min-Hash signatures for each user and use them to evaluate user similarity. Min-Hashing is a technique that approximates Jaccard Similarity by creating a set of hash functions and selecting the minimum hash value for each user. We will use a family of hash functions, ha,b(x) = (ax + b) mod R, where a and b are random integers and R is a large prime number.\n",
    "\n",
    "We will evaluate the Min-Hashing technique using **50**, **100**, and **200 hash functions**. For each value, we will output the pairs of users with an estimated similarity of at least **0.5**. Additionally, we will calculate the number of false positives and false negatives against the exact Jaccard Similarity. We will run the experiment five times using different hash functions and report the **average false positives and false negatives**. We will also analyze how the number of hash functions affects the false positive and false negative outputs.\n",
    "\n",
    "Codewise, we first import the necessary libraries, including `random` for generating random numbers and `numpy` for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa606c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8166c32",
   "metadata": {},
   "source": [
    "Then, we set a random seed to ensure reproducibility of the results. This means that each time the code is run, it will produce the same random numbers. \n",
    "\n",
    "Furthermore, we define a list of `num_hash_functions` containing the numbers **50**, **100**, and **200**, representing the different numbers of hash functions we want to evaluate. Additionally, we define a large prime number R to be used in the hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d02e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility of the same results.\n",
    "random.seed(50795)\n",
    "\n",
    "# Define the number of hash functions.\n",
    "num_hash_functions = [50, 100, 200]\n",
    "\n",
    "# Define the large prime number.\n",
    "R = 10000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4947948",
   "metadata": {},
   "source": [
    "Afterwards, we define a function `generate_hash_functions` that takes the number of hash functions `num_functions` as input and generates random values for `a` and `b` in the hash function *ha,b(x) = (ax + b) mod R*. The function returns a list of hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397c4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate hash functions.\n",
    "def generate_hash_functions(num_functions):\n",
    "    hash_functions = []\n",
    "    for _ in range(num_functions):\n",
    "        a = random.randint(1, R)\n",
    "        b = random.randint(0, R)\n",
    "        hash_functions.append((a, b))\n",
    "    return hash_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f3f28",
   "metadata": {},
   "source": [
    "In the following cell command, we create a dictionary `user_movies` to store the movies for each user and we define a function `compute_minhash_signature` that calculates the **Min-hash signature for a user**. It takes the `user_movies` (set of movie IDs) and `hash_functions` as input. \n",
    "\n",
    "The function initializes a signature list with infinity values and iterates over each `movie ID` for the user. For each hash function, it computes the hash value and updates the signature if the new hash value is *smaller*. The function returns the final `Min-hash signature` as *signature*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509a75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store movies for each user.\n",
    "user_movies = {}\n",
    "\n",
    "# Function to compute Min-hash signature for a user.\n",
    "def compute_minhash_signature(user_movies, hash_functions):\n",
    "    signature = [float('inf')] * len(hash_functions)\n",
    "    for movie_id in user_movies:\n",
    "        for i, (a, b) in enumerate(hash_functions):\n",
    "            hash_value = (a * movie_id + b) % R\n",
    "            if hash_value < signature[i]:\n",
    "                signature[i] = hash_value\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc48e29",
   "metadata": {},
   "source": [
    "We also create the `calculate_estimated_similarity` function, which is used within the evaluation process to compare the Min-hash signatures of different users and determine their estimated similarity. This function plays a crucial role in computing the similarity scores used to identify pairs of users with a similarity of at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd9de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate estimated similarity between users.\n",
    "def calculate_estimated_similarity(user1_signature, user2_signature):\n",
    "    num_matching = sum(user1_signature[i] == user2_signature[i] for i in range(len(user1_signature)))\n",
    "    return num_matching / len(user1_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c491375",
   "metadata": {},
   "source": [
    "As the next step, we iterate over the rows of the merged dataframe of the Task 1 (merged_df) and populate the dictionary by adding movie IDs to the corresponding user's set of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936c1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the merged_df rows to populate user_movies dictionary.\n",
    "for row in merged_df.itertuples():\n",
    "    userid = getattr(row, 'userid')\n",
    "    movieid = getattr(row, 'movieid')\n",
    "\n",
    "    if userid not in user_movies:\n",
    "        user_movies[userid] = set()\n",
    "    user_movies[userid].add(movieid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97820979",
   "metadata": {},
   "source": [
    "Then, we define the function `evaluate_similarity` that takes the number of hash functions `num_functions` as input. Within the function, we initialize variables for false positives and false negatives. The evaluation is run five times for averaging.\n",
    "\n",
    "Inside each evaluation run, we generate hash functions that was previously implemented using `generate_hash_functions`. We also create a dictionary `user_signatures` to store the *Min-hash signatures* for each user. We compute the Min-hash signatures using `compute_minhash_signature` for each user's set of movies.\n",
    "\n",
    "We then compare the Min-hash signatures and evaluate the similarity between users. We calculate the estimated similarity using `calculate_estimated_similarity` and the `exact Jaccard similarity`. Based on the comparison, we update the false positives and false negatives counters.\n",
    "\n",
    "After all evaluations, finally we calculate the average **false positives** and **false negatives** and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e51fb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate false positives and false negatives.\n",
    "def evaluate_similarity(num_functions):\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    # true_positives = 0\n",
    "    # true_negatives = 0\n",
    "\n",
    "    # Run the evaluation 5 times for averaging.\n",
    "    for _ in range(5):  \n",
    "\n",
    "        # Generate hash functions.\n",
    "        hash_functions = generate_hash_functions(num_functions)\n",
    "\n",
    "        # Create dictionaries to store Min-hash signatures.\n",
    "        user_signatures = {}\n",
    "\n",
    "        # Compute Min-hash signatures for each user.\n",
    "        for user_id, movies in user_movies.items():\n",
    "            signature = compute_minhash_signature(movies, hash_functions)\n",
    "            user_signatures[user_id] = signature\n",
    "\n",
    "        # Compare Min-hash signatures and evaluate similarity.\n",
    "        for user1_id, signature1 in user_signatures.items():\n",
    "            for user2_id, signature2 in user_signatures.items():\n",
    "                if user1_id != user2_id:\n",
    "                    estimated_similarity = calculate_estimated_similarity(signature1, signature2)\n",
    "                    exact_similarity = len(user_movies[user1_id].intersection(user_movies[user2_id])) / len(user_movies[user1_id].union(user_movies[user2_id]))\n",
    "                    if estimated_similarity >= 0.5 and exact_similarity < 0.5:\n",
    "                        false_positives += 1\n",
    "                    elif estimated_similarity < 0.5 and exact_similarity >= 0.5:\n",
    "                        false_negatives += 1\n",
    "                        \n",
    "                    # For evaluation purposes.\n",
    "                    #  elif estimated_similarity >= 0.5 and exact_similarity >= 0.5:\n",
    "                    #      true_positives += 1\n",
    "                    #  elif estimated_similarity < 0.5 and exact_similarity < 0.5:\n",
    "                    #      true_negatives += 1\n",
    "\n",
    "    # Divide by 5 for the average of the 5 runs and by 2 since we want to have a \n",
    "    # picture regarding the single and not duplicate comparisons of users.\n",
    "    avg_false_positives = false_positives / (2 * 5)\n",
    "    avg_false_negatives = false_negatives / (2 * 5) \n",
    "    \n",
    "    # For evaluation purposes.   \n",
    "    # true_positives = true_positives / (2 * 5)\n",
    "    # true_negatives = true_negatives / (2 * 5)\n",
    "    # return avg_false_positives, avg_false_negatives, true_positives, true_negatives\n",
    "\n",
    "    return avg_false_positives, avg_false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f70e7",
   "metadata": {},
   "source": [
    "Then, we perform the evaluation of Min-hashing for each number of hash functions in `num_hash_functions (50, 100, and 200)`. \n",
    "\n",
    "After that we call the `evaluate_similarity` function for each number of hash functions and retrieve the **average false positives and false negatives**.\n",
    "\n",
    "We then print the results in a readble format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed13098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hash Functions: 50\n",
      "Average False Positives: 204.4\n",
      "Average False Negatives: 1.8\n",
      "\n",
      "Number of Hash Functions: 100\n",
      "Average False Positives: 36.4\n",
      "Average False Negatives: 2.0\n",
      "\n",
      "Number of Hash Functions: 200\n",
      "Average False Positives: 10.0\n",
      "Average False Negatives: 2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the evaluation for each number of hash functions.\n",
    "for num_functions in num_hash_functions:\n",
    "    \n",
    "    # For evaluation purposes.   \n",
    "    # avg_false_positives, avg_false_negatives, true_positives, true_negatives = evaluate_similarity(num_functions)\n",
    "    \n",
    "    # Call the function implemented to find the pairs similatity.\n",
    "    avg_false_positives, avg_false_negatives = evaluate_similarity(num_functions)\n",
    "\n",
    "    print(\"Number of Hash Functions:\", num_functions)\n",
    "    print(\"Average False Positives:\", avg_false_positives)\n",
    "    print(\"Average False Negatives:\", avg_false_negatives)\n",
    "\n",
    "    # For evaluation purposes.   \n",
    "    # print(\"Average True Positives:\", true_positives)\n",
    "    # print(\"Average True Negatives:\", true_negatives)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26922e0",
   "metadata": {},
   "source": [
    "The results of the evaluation of Min-hash signatures with different numbers of hash functions are the following:\n",
    "\n",
    "*For 50 hash functions:*\n",
    "\n",
    "- Average False Positives: 204.4\n",
    "- Average False Negatives: 1.8\n",
    "\n",
    "*For 100 hash functions:*\n",
    "\n",
    "- Average False Positives: 36.4\n",
    "- Average False Negatives: 2.0\n",
    "\n",
    "*For 200 hash functions:*\n",
    "\n",
    "- Average False Positives: 10.0\n",
    "- Average False Negatives: 2.2\n",
    "\n",
    "The false positives represent the cases where the estimated similarity between users based on the Min-hash signatures is at least 0.5, but the exact Jaccard similarity is less than 0.5.\n",
    "\n",
    "On the other hand, false negatives represent the cases where the estimated similarity is less than 0.5, but the exact Jaccard similarity is at least 0.5.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "For **50** hash functions the average false positives are relatively **high at 204.4**, indicating that there is a significant number of pairs of users that have a high estimated similarity but a low exact Jaccard similarity. This suggests that the Min-hash signatures with 50 hash functions **may not be very accurate in capturing the true similarity between users**.\n",
    "\n",
    "The average false negatives are significantly low at **1.8**, indicating that the Min-hash signatures are generally effective in capturing the cases where users have a low estimated similarity but a high exact Jaccard similarity. The false negatives suggest that the Min-hash signatures perform well in identifying dissimilar users.\n",
    "\n",
    "When we elevate the number of hash functions to **100**, the average false positives **decrease significantly to 36.4 compared to 50 hash functions**. This indicates an improvement in accurately identifying pairs of users with high estimated similarity and low exact Jaccard similarity.\n",
    "The average false negatives **slightly rises to 2.0**, indicating that this non-substantial difference does not imply that the increase in the min hash signatures length deteriorates the non-idetifiable user pairs and consequently the amelioration in the the false pocitive incidents is a far more important improvement.\n",
    "\n",
    "Finally, when we set the number of hash functions to **200**, the average false positives further **decrease to 10.0, suggesting a higher accuracy in identifying pairs of users with high estimated similarity but low exact Jaccard similarity compared to 100 hash functions**.\n",
    "The average false negatives **slightly rises again to 2.2**, indicating that the Min-hash signatures continue to perform well in capturing cases of low estimated similarity but high exact Jaccard similarity.\n",
    "\n",
    "Overall, **increasing the number of hash functions from 50 to 200 leads to a reduction in false positives and to a non-substantial increase to false negatives that should not be taken into consideration as it is below one pair increase on average, resulting in more accurate similarity estimation overall using Min-hash signatures**. A larger number of hash functions provides finer-grained hashing and improves the precision of similarity estimation. However, it's important to *strike a balance as increasing the number of hash functions also increases the computational cost*. Therefore, choosing an appropriate number of hash functions requires considering the *trade-off between accuracy and computational efficiency*. \n",
    "\n",
    "Therefore, depending on the computation cost the 100 hash functions choice is probably the best one. However, we do not have any particular information and timewise demand so, the comparison lead to the same results, therefore we cannot select the optimum between the two, so we assume that the best one is the more efficient one - at least resultwise - so it is the solution containing the 200 hash functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2187b54",
   "metadata": {},
   "source": [
    "### Task 4: Locate Similar Users using LSH Index\n",
    "---\n",
    "\n",
    "In this task, we will implement Locality Sensitive Hashing (LSH) using a set of 200 hash functions. We will break the Min-Hash signatures into bands with a specific number of rows and specifically number of min hash values per band. Two instances of LSH will be used, namely:\n",
    "- LSH instance 1 with b = 25 and r = 8\n",
    "- LSH instance 2 with b = 40 and r = 5\n",
    "\n",
    "Using each LSH instance, we will locate pairs of users that share at least one identical band. These pairs present a high probabity of presenting similarity in a great extent. In the next step the range of the aforementioned user pairs are the only possible combinations that the Jaccard Similarity will be calculated so as to actually identify if they share similarity at least or above 50%. The LSH recommended similar pairs of users will be tested as regard as they exact Jaccards Similarity Index and the pairs with exact index value above 50% will be retained as true positives pairs that LSH method conducted. We will report the number of true pairs returned (true positives) and the number of similarity evaluations performed that correspond to the number of similar pairs the LSH method proposed as similar. The experiment will be repeated five times with different hash functions, and the averages of the true positives and similarity evaluations will be reported.\n",
    "\n",
    "Finally, based on the reported results, we will discuss the advantages and disadvantages of using LSH compared to directly comparing users based on their true representations sets.\n",
    "\n",
    "To be more precise, codewise we start by importing libraries and initialize components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eac0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility purposes.\n",
    "random.seed(129537555)\n",
    "\n",
    "# Set R to be a large prime number.\n",
    "R = 1000001\n",
    "\n",
    "# Set the similarity threshold.\n",
    "similarity_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921adfcf",
   "metadata": {},
   "source": [
    "As you can see from the above code, the random seed is set to ensure that the random numbers generated are reproducible and its value is set to `30031`. The variable R is assigned a constant value of **1000001**, which is a large prime number for the hash function slot limitation while simultaneously ensures the hash collision encounterment. The similarity threshold is set to **0.5** and it refers to the exact Jaccard Similarity Index and not the min hash signature index.\n",
    "\n",
    "The next step is the creation of the `hash_gen_init` function, which is used to generate min-hash signatures for users based on the movies they have seen. These min-hash signatures are later used for Locality Sensitive Hashing (LSH) to find similar users efficiently. The function takes three inputs: `data_frame`, which is the input dataframe, `num_hash_functions`, which represents the number of hash functions to generate, and `R`, which is a range limit used in the hash function calculations.\n",
    "\n",
    "More specifically, we first extract the distinct `userids` from the dataframe and store them in `distinct_user_ids`. Then, the dictionary `user_movies` is initialized to store the movies seen by each user, by iterating over each user and retrieving their respective movies from the dataframe, and then assigning the movies to the user in the dictionary.\n",
    "\n",
    "The below function also computes the min-hash signatures for each user. It iterates over the movies ids seen by each user, converts them to a set so as to have duplicates removed, and initializes the signature list with high values. For each movie, the function computes the hash values using the hash functions and updates the corresponding position in the signature list if the hash value is smaller - which will be for sure smaller from the initial ones that are out of bounce (R + 1). After computing the min-hash signature for a user, the signature list is appended to the `min_hash_signatures` list and finally the function returns the `min_hash_signatures` list containing the computed **min-hash signatures for all users**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a28eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_gen_init(data_frame, num_hash_functions, R):\n",
    "    \n",
    "    # Get distinct user ids from the imported dataframe.\n",
    "    distinct_user_ids = set(data_frame['userid'])\n",
    "\n",
    "    # Initialize a dictionary to store the movies seen by each user.\n",
    "    user_movies = {}\n",
    "\n",
    "    # Loop through users.\n",
    "    for user_id in distinct_user_ids:\n",
    "        \n",
    "        # Get the respective movies for the current user.\n",
    "        movies_seen = data_frame[data_frame['userid'] == user_id]['movieid']\n",
    "\n",
    "        # Assign the movies to the current user.\n",
    "        user_movies[user_id] = movies_seen.values\n",
    "\n",
    "    # Generate random values 'a' and 'b' for hash functions.\n",
    "    random_a = random.sample(range(1, R), num_hash_functions)\n",
    "    random_b = random.sample(range(0, R), num_hash_functions)\n",
    "\n",
    "    # Create a list of tuples representing the hash functions.\n",
    "    hash_functions = [(a, b) for a, b in zip(random_a, random_b)]\n",
    "\n",
    "    # Initialize a list to store the min-hash signatures for each user.\n",
    "    min_hash_signatures = []\n",
    "\n",
    "    # Compute min-hash signatures for each user.\n",
    "    for movies_seen in user_movies.values():\n",
    "        movies_seen = set(movies_seen)\n",
    "        \n",
    "        # Initialize signature with high values (could also be inf but R + 1 do the same).\n",
    "        signature = [R + 1 for _ in range(num_hash_functions)]  \n",
    "\n",
    "        # Compute the hash values and update the signature.\n",
    "        for movie_id in movies_seen:\n",
    "            for i, (a, b) in enumerate(hash_functions):\n",
    "                hash_value = (a * movie_id + b) % R\n",
    "                if hash_value < signature[i]:\n",
    "                    signature[i] = hash_value\n",
    "\n",
    "        min_hash_signatures.append(signature)\n",
    "\n",
    "    return min_hash_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21125a9",
   "metadata": {},
   "source": [
    "The purpose of this function is to prepare the data for LSH by generating min-hash signatures. LSH allows finding similar users efficiently by grouping them based on the similarity of their min-hash signatures in different bands and then assessing their true similarity using their initial representations. The task asks to perform LSH using two different instances, each with a specific number of bands and rows per band. The goal is to find pairs of users with a similarity of at least 0.5 and report the number of true positive pairs and the number of similarity evaluations performed. This process is repeated for 5 different runs using different generated hash functions.\n",
    "\n",
    "By using LSH and min-hash signatures, we gain efficiency in finding similar users by reducing the number of pairwise similarity evaluations. LSH allows us to narrow down the search space by focusing on users who have the same min-hash signatures in at least one band and as we learn through the lectures this significantly reduces the number of users' pairs that need to be compared for similarity evaluation. Thus, LSH provides an optimized probabilistic mechanism of finding similar users compared to directly comparing all possible users'pairs available in a dataset.\n",
    "\n",
    "After this, the next step is to create the `jaccard_similarity`, as we earlier defined it also, but not as a separate function. We choose to do so now in order to enhance the code's clarity and take advantage of the object oriented programming capabilities.\n",
    "\n",
    "To be more precise, the `jaccard_similarity` function computes the Jaccard coefficient between two sets. The function takes two input sets, set1 and set2, for which we want to calculate the Jaccard coefficient. It first finds the intersection of set1 and set2 using the intersection method, which returns a set containing the common elements between the two sets. It then finds the union of set1 and set2 using the union method, which returns a set containing all unique elements from both sets. The Jaccard coefficient is computed by dividing the length of the intersection set by the length of the union set. \n",
    "\n",
    "In other words, Jaccard similarity using the formula: \n",
    "\n",
    "`Jaccard Similarity = Intersection / Union`\n",
    "\n",
    "Finaly, the computed Jaccard coefficient is returned as the output of the function and is a measure of similarity between two sets and is calculated as the size of the intersection divided by the size of the union. It ranges from **0 to 1, where 0 indicates no similarity and 1 indicates complete similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f03145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "\n",
    "    # Compute the intersection of set1 and set2.\n",
    "    intersection = set1.intersection(set2)\n",
    "\n",
    "    # Compute the union of set1 and set2.\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    # Compute the Jaccard coefficient.\n",
    "    jaccard_coefficient = len(intersection) / len(union)\n",
    "    \n",
    "    return jaccard_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a28192",
   "metadata": {},
   "source": [
    "Afterwards, we implement a function to create hash tables for each band in the LSH. \n",
    "\n",
    "The generated hash tables will be used to efficiently identify potential candidate pairs of similar users based on the similarity of their min-hash signatures within a specific band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8210d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_tables(signature_matrix):\n",
    "\n",
    "    # Get the number of users and bands from the signature matrix.\n",
    "    num_users = signature_matrix.shape[0]\n",
    "    num_bands = signature_matrix.shape[1]\n",
    "\n",
    "    # Initialize an empty list of hash tables, one for each band.\n",
    "    hash_tables = [{} for _ in range(num_bands)]\n",
    "\n",
    "    # A for loop statement to iterate over each user.\n",
    "    for i in range(num_users):\n",
    "        \n",
    "        # A for loop statement to iterate over each band.\n",
    "        for b in range(num_bands):\n",
    "            \n",
    "            # Get the min-hash signature for the current band.\n",
    "            band = signature_matrix[i, b, :]\n",
    "\n",
    "            # Convert the signature to a hashable value.\n",
    "            hash_value = tuple(band)\n",
    "\n",
    "            # Check if the hash value exists in the hash table.\n",
    "            if hash_value not in hash_tables[b]:\n",
    "                \n",
    "                # If the hash value is not present, initialize an empty list.\n",
    "                hash_tables[b][hash_value] = []\n",
    "\n",
    "            # Append the current user to the corresponding hash table entry.\n",
    "            hash_tables[b][hash_value].append(i)\n",
    "\n",
    "    return hash_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3232ff",
   "metadata": {},
   "source": [
    "Then, we create the `evaluate_similarity` to find similar users based on their min-hash signatures and compute the number of true positives and similarity evaluations. The function gets as input the followings\n",
    "\n",
    "- user_movies: A dictionary mapping user IDs to the movies they have seen.\n",
    "- signature_matrix: The signature matrix containing min-hash signatures.\n",
    "- hash_tables: A list of hash tables, where each hash table corresponds to a band.\n",
    "- similarity_threshold: The threshold for similarity to consider two users as similar.\n",
    "\n",
    "And returns a dictionary of similar user pairs with their similarity values, the number of true positives, and the total number of similarity evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54905358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similarity(user_movies, signature_matrix, hash_tables, similarity_threshold):\n",
    "\n",
    "    # Get the number of users and bands from the signature matrix.\n",
    "    num_users = signature_matrix.shape[0]\n",
    "    num_bands = signature_matrix.shape[1]\n",
    "\n",
    "    # Initialize variables to store the results.\n",
    "    similar_users = {}\n",
    "    true_positives = 0\n",
    "    similarity_evaluations = 0\n",
    "\n",
    "    # Iterate over each pair of users.\n",
    "    for i in range(num_users):\n",
    "        for j in range(i + 1, num_users):\n",
    "            \n",
    "            # Iterate over each band.\n",
    "            for b in range(num_bands):\n",
    "                \n",
    "                # Get the min-hash signature for the current user and band.\n",
    "                band = signature_matrix[i, b, :]\n",
    "\n",
    "                # Convert the signature to a hashable value.\n",
    "                hash_value = tuple(band)\n",
    "\n",
    "                # Check if the hash value exists in the hash table for the current band.\n",
    "                if hash_value in hash_tables[b]:\n",
    "                    \n",
    "                    # Check if the second user is in the list of potential similar users\n",
    "                    if j in hash_tables[b][hash_value]:\n",
    "                        \n",
    "                        # Compute the Jaccard similarity between the two users.\n",
    "                        s1 = set(user_movies[i + 1])\n",
    "                        s2 = set(user_movies[j + 1])\n",
    "                        similarity = jaccard_similarity(s1, s2)\n",
    "                        similarity_evaluations += 1\n",
    "\n",
    "                        # Check if the similarity exceeds the threshold.\n",
    "                        if similarity >= similarity_threshold:\n",
    "                            \n",
    "                            # Update the true positives count and store the similar user pair.\n",
    "                            true_positives += 1\n",
    "                            key = str(i + 1) + \" - \" + str(j + 1)\n",
    "                            similar_users[key] = similarity\n",
    "\n",
    "    # Sort the similar user pairs based on similarity in descending order.\n",
    "    similar_users = dict(sorted(similar_users.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Return the results.\n",
    "    return similar_users, true_positives, similarity_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b29d95",
   "metadata": {},
   "source": [
    "Afterwards, we implement the function `user_similarity_lsh` in order to compute user similarity using Locality Sensitive Hashing (LSH) algorithm. This functions takes as required inputs the dictionary mapping user IDs to the movies they have seen, `user_movies`, the given number of bands for LSH `num_bands`, the also given number of rows per band for LSH, `num_rows_per_band`, the large prime number `R`, and the threshold for similarity to consider two users as similar - 0.5.\n",
    "\n",
    "The following function will output a dictionary of similar user pairs with their similarity values, the number of true positives, and the total number of similarity evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8188b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_similarity_lsh(user_movies, num_bands, num_rows_per_band, R, similarity_threshold):\n",
    "    \n",
    "    # Calculate the number of hash functions based on the number of bands and rows per band.\n",
    "    num_hash_functions = num_bands * num_rows_per_band\n",
    "\n",
    "    # Generate min-hash signatures for users.\n",
    "    user_signatures = hash_gen_init(ratings_df, num_hash_functions, R)\n",
    "\n",
    "    # Reshape the user signatures into a signature matrix.\n",
    "    signature_matrix = np.array(user_signatures).reshape(-1, num_bands, num_rows_per_band)\n",
    "\n",
    "    # Create hash tables using the signature matrix.\n",
    "    hash_tables = create_hash_tables(signature_matrix)\n",
    "\n",
    "    # Evaluate user similarity using LSH.\n",
    "    similar_users, true_positives, similarity_evaluations = evaluate_similarity(user_movies, signature_matrix, hash_tables, similarity_threshold)\n",
    "\n",
    "    # Return the results.\n",
    "    return similar_users, true_positives, similarity_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109ab10",
   "metadata": {},
   "source": [
    "Finally, we are now ready to execute the LSH Instance 1 with the specified parameters, compute the similar user pairs, and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5916fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Instance 1 - Similar Users:\n",
      "\n",
      "Users 408 - 898: 0.8387096774193549\n",
      "Users 328 - 788: 0.6729559748427673\n",
      "Users 489 - 587: 0.6299212598425197\n",
      "Users 674 - 879: 0.5217391304347826\n",
      "Users 554 - 764: 0.5170068027210885\n"
     ]
    }
   ],
   "source": [
    "# LSH Instance 1: b = 25, r = 8.\n",
    "num_bands_1 = 25\n",
    "num_rows_per_band_1 = 8\n",
    "\n",
    "# Compute user similarity using LSH Instance 1.\n",
    "similar_users_1, true_positives_1, similarity_evaluations_1 = user_similarity_lsh(user_movies, num_bands_1, num_rows_per_band_1, R, similarity_threshold)\n",
    "\n",
    "# Print init mesage.\n",
    "print(\"LSH Instance 1 - Similar Users:\\n\")\n",
    "\n",
    "# Print similar user pairs and their similarity values for LSH Instance 1 - last run.\n",
    "for key, value in similar_users_1.items():\n",
    "    print(\"Users {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd88e7",
   "metadata": {},
   "source": [
    "And the average results for 5 runs for the LSH Instance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c048d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Instance 1 - True Positives: 2.8\n",
      "LSH Instance 1 - Similarity Evaluations: 45\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of true positives for LSH Instance 1\n",
    "print(\"LSH Instance 1 - True Positives:\", true_positives_1/5)\n",
    "\n",
    "# Print the total number of similarity evaluations for LSH Instance 1\n",
    "print(\"LSH Instance 1 - Similarity Evaluations:\", similarity_evaluations_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a138fd",
   "metadata": {},
   "source": [
    "We follow the same procedure for the LSH Instance 2 with it's own specified parameters, and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d5714a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Instance 2 - Similar Users:\n",
      "\n",
      "Users 408 - 898: 0.8387096774193549\n",
      "Users 328 - 788: 0.6729559748427673\n",
      "Users 489 - 587: 0.6299212598425197\n",
      "Users 600 - 826: 0.5454545454545454\n",
      "Users 451 - 489: 0.5333333333333333\n",
      "Users 554 - 764: 0.5170068027210885\n",
      "Users 197 - 826: 0.512987012987013\n",
      "Users 197 - 600: 0.5\n",
      "Users 800 - 879: 0.5\n"
     ]
    }
   ],
   "source": [
    "# LSH Instance 2: b = 40, r = 5.\n",
    "num_bands_2 = 40\n",
    "num_rows_per_band_2 = 5\n",
    "\n",
    "# Compute user similarity using LSH Instance 2.\n",
    "similar_users_2, true_positives_2, similarity_evaluations_2 = user_similarity_lsh(user_movies, num_bands_2, num_rows_per_band_2, R, similarity_threshold)\n",
    "\n",
    "# Print init message.\n",
    "print(\"LSH Instance 2 - Similar Users:\\n\")\n",
    "\n",
    "# Print similar user pairs and their similarity values for LSH Instance 2 - last run.\n",
    "for key, value in similar_users_2.items():\n",
    "    print(\"Users {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb816a1",
   "metadata": {},
   "source": [
    "Last but not least, we print the average results for 5 runs for the LSH Instance 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "436b5628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH Instance 2 - True Positives: 8.8\n",
      "LSH Instance 2 - Similarity Evaluations: 1998\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of true positives for LSH Instance 2.\n",
    "print(\"LSH Instance 2 - True Positives:\", true_positives_2 / 5)\n",
    "\n",
    "# Print the total number of similarity evaluations for LSH Instance 2.\n",
    "print(\"LSH Instance 2 - Similarity Evaluations:\", similarity_evaluations_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62a79",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "---\n",
    "\n",
    "LSH (Locality-Sensitive Hashing) is a technique used in data mining and similarity search to efficiently approximate similarity between high-dimensional data points. It is particularly useful for handling big datasets because it allows for scalable and efficient nearest neighbor search.\n",
    "\n",
    "The main idea behind LSH is to hash data points in such a way that similar points have a high probability of being hashed into the same or nearby hash buckets. This enables the identification of potential similar pairs by focusing on a subset of the data instead of comparing all pairs, which is computationally expensive for large datasets. LSH is particularly effective for high-dimensional data because the curse of dimensionality makes traditional distance metrics less effective. In high-dimensional spaces, the density of data points becomes sparse, and the notion of distance breaks down. LSH helps mitigate this issue by focusing on the local structure of data points rather than global distances.\n",
    "\n",
    "The main characteristics of hypertunning parameters of the LSH method is the number of bands and the number of rows in each band the signatures vectors will be divided to as this parameter determines the trade-off between the similarity accuracy attained and the complexity augmentation because of the increase in the possible simmilar pairs and consequently the similarity computations that are needed. By increasing  in LSH increases recall by allowing more potential matches with lower similarity scores. By sIncreasing  makes the match criteria stricter, leading to higher precision by restricting matches to higher similarity scores. These parameters can be adjusted to balance the trade-off between recall and precision based on the specific requirements of the application or similarity search task at hand.\n",
    "\n",
    "In our solutions:\n",
    "* *LHS Instance 1: b = 25, r = 8*\n",
    "\n",
    "There are 45 similarity evaluations on average so the algorithm proposed **45** user pairs on average depending on 8 integers long tuple bands where a pair shares at least one band in common. From these evaluations about **3** pairs are identified correclty and are truly similar pairs over the 10 actual similar pairs identified form the initial set representations. The accuracy attained are rather low and this indicated that the bands and the presimilarity requirements are very strict and the pairs' selection is very stricted and limited.\n",
    "\n",
    "\n",
    "* *LHS Instance 2: b = 40, r = 5*\n",
    "\n",
    "As the bands' number increases and the number of rows in each band decreases , the recognizable pattern among bands becomes smaller and more often identified so the similarity evaluations augments and reaches **1998** for these trial pairs the true positives identified among them reaches the **8.8** pairs on average out of the 10 actual similar pairs with similarity above the threshold set. The computational complexity is increased as the possible pairs are increased but on the other hand the accuracy and the efficiency attained is increased.\n",
    "\n",
    "**Therefore, by using LSH in enormous datasets, we gain computational speed at the cost of some accuracy and by decreasing the b parameter (or increasing the r parameter) we save computational force at the cost of the some accuracy in the actual similarity identifications.**\n",
    "\n",
    "**Finally, we need to point out that the results of our analysis heavily depend on the set seed. During the development of the current notebook, we have found out for the first LSH instance values as low as 1.6 to 3.8 and for the second one from 7.6 to 9.4. However, we choose to present a mediocre output, in order to have a more fair analysis.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
